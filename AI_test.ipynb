{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 30.9067\n",
      "Epoch [2/20], Loss: 27.2627\n",
      "Epoch [3/20], Loss: 25.9991\n",
      "Epoch [4/20], Loss: 25.6579\n",
      "Epoch [5/20], Loss: 26.0223\n",
      "Epoch [6/20], Loss: 25.0183\n",
      "Epoch [7/20], Loss: 24.7995\n",
      "Epoch [8/20], Loss: 24.6844\n",
      "Epoch [9/20], Loss: 24.6942\n",
      "Epoch [10/20], Loss: 24.4605\n",
      "Epoch [11/20], Loss: 24.3531\n",
      "Epoch [12/20], Loss: 24.1502\n",
      "Epoch [13/20], Loss: 24.0782\n",
      "Epoch [14/20], Loss: 24.0226\n",
      "Epoch [15/20], Loss: 23.9732\n",
      "Epoch [16/20], Loss: 23.9302\n",
      "Epoch [17/20], Loss: 23.8866\n",
      "Epoch [18/20], Loss: 23.8481\n",
      "Epoch [19/20], Loss: 23.8115\n",
      "Epoch [20/20], Loss: 23.7755\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import ToTensor\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import onnx\n",
    "\n",
    "# Custom Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, input_dir, target_dir):\n",
    "        self.input_files = sorted([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith('_object_noise.tif')], key=lambda x: int(os.path.basename(x).split('_')[0]))\n",
    "        self.target_files = sorted([os.path.join(target_dir, f) for f in os.listdir(target_dir) if f.endswith('_noise.tif')], key=lambda x: int(os.path.basename(x).split('_')[0]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        input_image = ToTensor()(np.array(Image.open(self.input_files[idx]), dtype=np.float32) )\n",
    "        target_image = ToTensor()(np.array(Image.open(self.target_files[idx]), dtype=np.float32) )\n",
    "        return input_image, target_image\n",
    "\n",
    "# ResNet-like Neural Network\n",
    "class SimpleResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleResNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv6 = nn.Conv2d(64, 1, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.relu(self.conv4(x))\n",
    "        x = self.relu(self.conv5(x))\n",
    "        x = self.conv6(x)\n",
    "        return x + residual\n",
    "\n",
    "# Parameters\n",
    "input_dir = r\"E:\\Deeplearning\\C11800\\High\\N2N\\object_noise\"\n",
    "target_dir = r\"E:\\Deeplearning\\C11800\\High\\N2N\\noise\"\n",
    "batch_size = 32\n",
    "epochs = 40\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Dataset and DataLoader\n",
    "dataset = CustomDataset(input_dir, target_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = SimpleResNet().to(device)\n",
    "\n",
    "criterion = nn.L1Loss()  # MAE\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=15, gamma=0.1)\n",
    "\n",
    "# Training Loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    scheduler.step()\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {epoch_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# Save Model as ONNX\n",
    "dummy_input = torch.randn(1, 1, 256, 256).to(device)\n",
    "torch.onnx.export(model, dummy_input, \"resnet_model.onnx\", opset_version=11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Inference\n",
    "model.eval()\n",
    "test_image_path = \"path_to_test_image.tif\"\n",
    "\n",
    "try:\n",
    "    test_image = ToTensor()(np.array(Image.open(test_image_path), dtype=np.uint16) / 65535.0).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(test_image)\n",
    "\n",
    "    output_image = output.squeeze().cpu().numpy()\n",
    "    output_image = np.clip(output_image * 65535, 0, 65535).astype(np.uint16)\n",
    "    Image.fromarray(output_image).save(\"output_image.tif\")\n",
    "    print(\"Inference complete. Output saved as 'output_image.tif'.\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {test_image_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during inference: {e}\")\n",
    "\n",
    "print(\"Training and inference complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'OnnxModel' object has no attribute 'prune'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m OnnxModel(onnx\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Prune unnecessary nodes (example: remove identity nodes)\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprune\u001b[49m()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Save the pruned model\u001b[39;00m\n\u001b[0;32m     14\u001b[0m onnx\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mmodel, pruned_model_path)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'OnnxModel' object has no attribute 'prune'"
     ]
    }
   ],
   "source": [
    "import onnx\n",
    "from onnxruntime.transformers.onnx_model import OnnxModel\n",
    "\n",
    "# Load ONNX model\n",
    "model_path = \"resnet_model.onnx\"\n",
    "pruned_model_path = \"resnet_model_pruned.onnx\"\n",
    "\n",
    "model = OnnxModel(onnx.load(model_path))\n",
    "\n",
    "# Prune unnecessary nodes (example: remove identity nodes)\n",
    "model.prune()\n",
    "\n",
    "# Save the pruned model\n",
    "onnx.save(model.model, pruned_model_path)\n",
    "print(f\"Pruned model saved to {pruned_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "Unable to open proto file: resnet_model_pruned.onnx. Please check if it is a valid proto. ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m quantized_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresnet_model_quantized.onnx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Perform dynamic quantization\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[43mquantize_dynamic\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpruned_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantized_model_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mQuantType\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mQInt8\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Quantize weights to INT8\u001b[39;49;00m\n\u001b[0;32m     12\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQuantized model saved to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquantized_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\to5568.ADIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\quantization\\quantize.py:832\u001b[0m, in \u001b[0;36mquantize_dynamic\u001b[1;34m(model_input, model_output, op_types_to_quantize, per_channel, reduce_range, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, extra_options)\u001b[0m\n\u001b[0;32m    826\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m op_types_to_quantize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(op_types_to_quantize) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    827\u001b[0m     op_types_to_quantize \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(IntegerOpsRegistry\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m    829\u001b[0m model \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    830\u001b[0m     save_and_reload_model_with_shape_infer(model_input)\n\u001b[0;32m    831\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_input, onnx\u001b[38;5;241m.\u001b[39mModelProto)\n\u001b[1;32m--> 832\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mload_model_with_shape_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m )\n\u001b[0;32m    835\u001b[0m pre_processed: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m model_has_pre_process_metadata(model)\n\u001b[0;32m    836\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pre_processed:\n",
      "File \u001b[1;32mc:\\Users\\to5568.ADIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnxruntime\\quantization\\quant_utils.py:982\u001b[0m, in \u001b[0;36mload_model_with_shape_infer\u001b[1;34m(model_path)\u001b[0m\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model_with_shape_infer\u001b[39m(model_path: Path) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ModelProto:\n\u001b[0;32m    981\u001b[0m     inferred_model_path \u001b[38;5;241m=\u001b[39m generate_identified_filename(model_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-inferred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 982\u001b[0m     \u001b[43monnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape_inference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minferred_model_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m     model \u001b[38;5;241m=\u001b[39m onnx\u001b[38;5;241m.\u001b[39mload(inferred_model_path\u001b[38;5;241m.\u001b[39mas_posix())\n\u001b[0;32m    984\u001b[0m     add_infer_metadata(model)\n",
      "File \u001b[1;32mc:\\Users\\to5568.ADIR\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\onnx\\shape_inference.py:96\u001b[0m, in \u001b[0;36minfer_shapes_path\u001b[1;34m(model_path, output_path, check_type, strict_mode, data_prop)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     95\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m model_path\n\u001b[1;32m---> 96\u001b[0m \u001b[43mC\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfer_shapes_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_prop\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValidationError\u001b[0m: Unable to open proto file: resnet_model_pruned.onnx. Please check if it is a valid proto. "
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "# Input and output model paths\n",
    "pruned_model_path = \"resnet_model_pruned.onnx\"\n",
    "quantized_model_path = \"resnet_model_quantized.onnx\"\n",
    "\n",
    "# Perform dynamic quantization\n",
    "quantize_dynamic(\n",
    "    model_input=pruned_model_path,\n",
    "    model_output=quantized_model_path,\n",
    "    weight_type=QuantType.QInt8  # Quantize weights to INT8\n",
    ")\n",
    "\n",
    "print(f\"Quantized model saved to {quantized_model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
